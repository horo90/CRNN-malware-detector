import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import pickle
from utils import *
from tensorflow.contrib.layers import batch_norm
from sklearn.naive_bayes import GaussianNB
from LSTMCell import LSTMCell


class DetectionModel:
    def __init__(self, encode_len, n_voca, n_embed, n_rnn_hidden):
        self.encode_len = encode_len
        self.n_voca = n_voca
        self.n_embed = n_embed
        self.n_rnn_hidden = n_rnn_hidden

    def deconv(self, x, W, output_shape, name):
        return tf.nn.conv2d_transpose(x, W, strides=[1, 1, 1, 1], output_shape=output_shape, padding='SAME', name=name)

    def max_pool(self, x, name):
        return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name=name)

    def conv2d(self, x, W, name):
        return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME', name=name)

    def avg_pool(self, x, name):
        return tf.nn.avg_pool(x, ksize=[1, 2, 2, 1], strides=[1, 1, 1, 1], padding='SAME', name=name)

    def upsample(self, x, factor=[2, 2], name=None):
        size = [int(x.shape[1] * factor[0]), int(x.shape[2] * factor[1])]
        return tf.image.resize_bilinear(x, size=size, align_corners=None, name=name)

    def build_encoder(self, x_embed, batch_size):
        print('[*] build a Convolutiona Words Encoder')
        with tf.variable_scope('encoder', reuse=tf.AUTO_REUSE):
            W_conv1 = tf.Variable(tf.random_normal([4, self.n_embed, 1, 5], stddev=0.1))
            b_conv1 = tf.Variable(tf.zeros([5]))
            W_conv2 = tf.Variable(tf.random_normal([2, int(self.n_embed / 2), 5, 3], stddev=0.1))
            b_conv2 = tf.Variable(tf.zeros([3]))
            W_fc1 = tf.Variable(tf.random_normal([int(self.encode_len / 4) * int(self.n_embed / 4) * 3, 100], stddev=0.1))
            b_fc1 = tf.Variable(tf.zeros([100]))

            z1 = tf.nn.relu(batch_norm(self.conv2d(x_embed, W_conv1, name='conv1') + b_conv1))
            z1 = self.max_pool(z1, name='max_pool1')
            z2 = tf.nn.relu(batch_norm(self.conv2d(z1, W_conv2, name='conv2') + b_conv2))
            z2 = self.max_pool(z2, name='max_pool2')
            z2 = tf.reshape(z2, [batch_size, int(self.encode_len / 4) * int(self.n_embed / 4) * 3])
            fc1 = tf.nn.relu(batch_norm(tf.matmul(z2, W_fc1) + b_fc1))

            return fc1

    def build_decoer(self, z, batch_size):
        print('[*] build a Convolutional Words Decoder')
        with tf.variable_scope('decoder', reuse=tf.AUTO_REUSE):
            W_fc1 = tf.Variable(tf.random_normal([100, int(self.encode_len / 4) * int(self.n_embed / 4) * 3], stddev=0.1))
            b_fc1 = tf.Variable(tf.zeros([int(self.encode_len / 4) * int(self.n_embed / 4) * 3]))
            W_conv1 = tf.Variable(tf.random_normal([2, int(self.n_embed / 2), 3, 5], stddev=0.1))
            b_conv1 = tf.Variable(tf.zeros([5]))
            W_conv2 = tf.Variable(tf.random_normal([4, self.n_embed, 5, 1], stddev=0.1))
            b_conv2 = tf.Variable(tf.zeros([1]))

            fc1 = tf.nn.relu(batch_norm(tf.matmul(z, W_fc1) + b_fc1))
            fc1 = tf.reshape(fc1, [batch_size, int(self.encode_len / 4), int(self.n_embed / 4), 3])
            z1 = self.upsample(fc1, name='upsample1')
            z1 = tf.nn.relu(batch_norm(self.conv2d(z1, W_conv1, name='conv1') + b_conv1))
            z2 = self.upsample(z1, name='upsample2')
            z2 = tf.nn.relu(batch_norm(self.conv2d(z2, W_conv2, name='conv2') + b_conv2))

            return z2

    def build_drnn_classifier(self, rnn_input, lens, batch_size, time_step):
        W_out = tf.Variable(tf.random_normal([self.n_rnn_hidden, 1], stddev=0.1))
        b_out = tf.Variable(tf.zeros([1]))

        cell = LSTMCell(100, self.n_rnn_hidden, forget_bias=1., use_peephole=True)

        h_init = tf.zeros([batch_size, self.n_rnn_hidden])
        c_init = tf.zeros([batch_size, self.n_rnn_hidden])
        index = tf.constant(0, tf.int32)

        def rnn_cond(i, ts, h, c, ouptuts, cands, forgets):
            return tf.less(i, ts)

        def rnn_body(i, ts, h, c, outputs, cands, forgets):

            x = rnn_input[:,i]
            h, c, cand, f = cell.forward(x, h, c)
            outputs = outputs.write(i, h)
            cands = cands.write(i, cand)
            forgets = forgets.write(i, f)

            i = tf.add(i, 1)
            return i, ts, h, c, outputs, cands, forgets

        _, _, h, c, output_arr, cands_arr, forgets_arr = tf.while_loop(rnn_cond, rnn_body,
                      [index,
                       time_step,
                       h_init,
                       c_init,
                       tf.TensorArray(dtype=tf.float32, size=time_step),
                       tf.TensorArray(dtype=tf.float32, size=time_step),        # cands
                       tf.TensorArray(dtype=tf.float32, size=time_step)])       # forgets

        raw_ind = tf.reshape(tf.range(batch_size), [batch_size, 1])
        lens = tf.reshape(lens, [batch_size, 1])
        ind = tf.concat([raw_ind, lens], 1)

        outputs = tf.gather_nd(tf.transpose(output_arr.stack(), [1, 0, 2]), ind)
        logits = tf.matmul(outputs, W_out) + b_out

        return logits, cands_arr.stack(), forgets_arr.stack()

    def build(self):
        self.x = tf.placeholder(tf.int32, [None, None, self.encode_len])
        self.ae_lr = tf.placeholder(tf.float32, [])
        self.drnn_lr = tf.placeholder(tf.float32, [])
        self.actual_len = tf.placeholder(tf.int32, [None])
        self.target = tf.placeholder(tf.float32, [None, 1])
        batch_size, sub_len, _ = tf.unstack(tf.shape(self.x))
        x = tf.reshape(self.x, [batch_size * sub_len, self.encode_len])

        x_target = tf.one_hot(x, self.n_voca + 1, dtype=tf.float32)
        x_target = tf.expand_dims(x_target, 3)   # Tensor("ExpandDims:0", shape=(?, 16, 31, 1), dtype=float32)

        embeddings = tf.Variable(tf.random_uniform([self.n_voca + 1, self.n_embed]))
        x_embed = tf.nn.embedding_lookup(embeddings, x)
        x_embed = tf.expand_dims(x_embed, 3)   # Tensor("ExpandDims_1:0", shape=(?, 16, 100, 1), dtype=float32)

        # Opcode-level Convolutional Autoencoder
        z = self.build_encoder(x_embed, batch_size * sub_len)
        out = self.build_decoer(z, batch_size * sub_len)

        # Opcode-level CAE input reconstruction
        out = tf.reshape(out, [batch_size * sub_len * self.encode_len, 100])
        W_fc = tf.Variable(tf.random_normal([self.n_embed, self.n_voca + 1], stddev=0.1))
        b_fc = tf.Variable(tf.zeros([self.n_voca + 1]))
        out_fc = tf.matmul(out, W_fc) + b_fc
        x_recon = tf.reshape(out_fc, [batch_size * sub_len, self.encode_len, self.n_voca + 1])
        x_target = tf.reshape(x_target, [batch_size * sub_len, self.encode_len, self.n_voca + 1])

        # for user
        self.x_target = tf.reshape(x_target, [batch_size, sub_len, self.encode_len, self.n_voca + 1])
        self.x_recon = tf.reshape(tf.nn.softmax(out_fc), [batch_size, sub_len, self.encode_len, self.n_voca + 1])

        # Dynamic rnn classifier
        drnn_x = tf.reshape(z, [batch_size, sub_len, 100])
        self.drnn_x = drnn_x
        logits, cands, forgets = self.build_drnn_classifier(drnn_x, self.actual_len, batch_size, sub_len)
        self.cands = tf.reduce_mean(tf.transpose(cands, [1, 0, 2]), 2)
        self.forgets = tf.reduce_mean(tf.transpose(forgets, [1, 0, 2]), 2)
        self.logits = tf.nn.sigmoid(logits)

        # for training
        self.ae_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=x_recon, labels=x_target, dim=2))
        self.ae_train = tf.train.AdamOptimizer(self.ae_lr).minimize(self.ae_loss)

        self.drnn_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=self.target))
        self.drnn_train = tf.train.AdamOptimizer(self.drnn_lr).minimize(self.drnn_loss)

def padding(data, actual_lens, encoded_len):
    max_len = np.max(actual_lens)
    res = encoded_len - max_len % encoded_len
    max_len += res

    padded_data = list()
    padded_lens = list()
    for i in range(len(data)):
        pad = np.zeros((max_len - actual_lens[i]), dtype=np.int32)
        padd_arr = np.concatenate((data[i], pad))
        padded_lens.append(actual_lens[i] + encoded_len - actual_lens[i] % encoded_len)
        padded_data.append(padd_arr)
    padded_data = np.array(padded_data)
    padded_lens = np.array(padded_lens)
    return padded_data, padded_lens, max_len

encoded_len = 16
n_voca = 30
n_embed = 100
n_rnn_hidden = 1024
lr = 1e-2

def train_linear_classifier(sess, model, tr_data, tr_labels, tr_lens, n_voca, encode_len, n_minibatch, model_path, strategy, k, n_sampled_paths):
    print('[*] train the linear classifier')
    if not os.path.exists(model_path):
        print('train model first')
        return

    print('[*] restore the trained model')
    saver = tf.train.Saver()
    saver.restore(sess, model_path + '/model.ckpt')

    confidences = list()
    labels = list()
    for i in range(len(tr_data)):
        tr_x = tr_data[i]
        tr_y = tr_labels[i]
        tr_l = tr_lens[i]

        tr_x, tr_l, tr_max_len = padding4(tr_x, tr_l, encode_len)
        tr_x = tr_x.reshape((-1, int(tr_max_len / encode_len), encode_len))
        tr_l = (tr_l / encode_len).astype(np.int32)
        actual = int(tr_y[0, 0])

        logits, cands, forgets = sess.run([model.logits, model.cands, model.forgets],
                                          feed_dict={model.x: tr_x, model.actual_len: tr_l})
        confidence = np.mean(logits.reshape((-1)))
        confidences.append(confidence)
        labels.append(actual)
    confidences = np.array(confidences).reshape((-1, 1))
    labels = np.array(labels).reshape((-1))

    gnb = GaussianNB(priors=[0.5, 0.5])
    gnb.fit(confidences, labels)
    print(strategy, k, gnb.score(confidences, labels))

    if not os.path.exists('.\\linear_classifier_' + strategy + '_' + str(n_sampled_paths)):
        os.mkdir('.\\linear_classifier_' + strategy + '_' + str(n_sampled_paths))

    pickle.dump(gnb, open('.\\linear_classifier_' + strategy + '_' + str(n_sampled_paths) + '\\cls_' + str(k), 'wb'))

def train(sess, model, tr_data, tr_labels, tr_lens, n_voca, encode_len, n_minibatch, ae_lr, rnn_lr, model_path, n_update_ae, n_update_rnn, strategy, k):
    print('[*] training a Malware Detection Model')
    saver = tf.train.Saver()
    sess.run(tf.global_variables_initializer())

    tr_data = tr_data.reshape((-1))
    tr_labels = tr_labels.reshape((-1, 1))
    tr_lens = tr_lens.reshape((-1))
    index = np.arange(len(tr_data))
    np.random.shuffle(index)
    tr_data = tr_data[index]
    tr_labels = tr_labels[index]
    tr_lens = tr_lens[index]

    print('[*] training a Convolutional Words Autoencoder (', encode_len,')')
    m = 0
    ae_xs = list()
    ae_losses = list()
    for i in range(n_update_ae):
        tr_x = tr_data[m * n_minibatch : (m+1) * n_minibatch]
        tr_l = tr_lens[m * n_minibatch : (m+1) * n_minibatch]
        tr_y = tr_labels[m * n_minibatch : (m+1) * n_minibatch]
        m += 1
        if m * n_minibatch >= len(tr_data):m = 0

        tr_x, tr_l, tr_max_len = padding4(tr_x, tr_l, encode_len)
        tr_x = tr_x.reshape((-1, int(tr_max_len / encoded_len), encoded_len))
        tr_l = (tr_l / encoded_len).astype(np.int32)

        ae_loss, _, target, recon = sess.run([model.ae_loss, model.ae_train, model.x_target, model.x_recon], feed_dict={model.x:tr_x, model.ae_lr:ae_lr})

        if i % 10 == 0:
            print('[', k, '] ae', i, ae_loss)
            ae_xs.append(i)
            ae_losses.append(ae_loss)
    ################################################################

    print('[*] training Dynamic RNN classifier using encoded opcodes')
    m = 0
    rnn_xs = list()
    rnn_losses = list()
    # fig, ax = plt.subplots(1)
    for i in range(n_update_rnn):
        tr_x = tr_data[m * n_minibatch: (m + 1) * n_minibatch]
        tr_l = tr_lens[m * n_minibatch: (m + 1) * n_minibatch]
        tr_y = tr_labels[m * n_minibatch: (m + 1) * n_minibatch]
        m += 1
        if m * n_minibatch >= len(tr_data): m = 0

        tr_x, tr_l, tr_max_len = padding4(tr_x, tr_l, encode_len)
        tr_x = tr_x.reshape((-1, int(tr_max_len / encoded_len), encoded_len))
        tr_l = (tr_l / encoded_len).astype(np.int32)

        loss, _ = sess.run([model.drnn_loss, model.drnn_train], feed_dict={model.x:tr_x, model.target:tr_y, model.actual_len:tr_l, model.drnn_lr:rnn_lr})

        if i % 10 == 0:
            print('[', k, '] rnn', i, loss)
            rnn_xs.append(i)
            rnn_losses.append(loss)

    print('[*] Saving the trained model')
    if not os.path.exists(model_path):
        os.mkdir(model_path)
    saver.save(sess, model_path + '/model.ckpt')

    f = open('./result_' + strategy + '/train_result_' + str(k) + '.txt', 'w')
    f.write('['+ str(k) + '] ae xs\n')
    f.write(str(ae_xs) + '\n')
    f.write('['+ str(k) + '] ae losses\n')
    f.write(str(ae_losses) + '\n')
    f.write('['+ str(k) + '] rnn xs\n')
    f.write(str(rnn_xs) + '\n')
    f.write('['+ str(k) + '] rnn losses\n')
    f.write(str(rnn_losses) + '\n')
    f.close()


def test(sess, model, te_data, te_labels, te_lens, n_sampled_paths, encode_len, model_path, strategy, k):
    print('[*] testing the model performance')
    if not os.path.exists(model_path):
        print('train model first')
        return

    print('[*] restore the trained model')
    saver = tf.train.Saver()
    saver.restore(sess, model_path + '/model.ckpt')

    print('[*] make dir for saving activation')
    if not os.path.exists('.\\input-activations_' + strategy + '_' + str(n_sampled_paths)):
        os.mkdir('.\\input-activations_' + strategy + '_' + str(n_sampled_paths))
    if not os.path.exists('.\\forget-activations_' + strategy + '_' + str(n_sampled_paths)):
        os.mkdir('.\\forget-activations_' + strategy + '_' + str(n_sampled_paths))
    if not os.path.exists('.\\test-files_' + strategy + '_' + str(n_sampled_paths)):
        os.mkdir('.\\test-files_' + strategy + '_' + str(n_sampled_paths))
    if not os.path.exists('.\\logits_' + strategy + '_' + str(n_sampled_paths)):
        os.mkdir('.\\logits_' + strategy + '_' + str(n_sampled_paths))

    print('[*] classifying test dataset')
    logits_list = list()
    confidences = list()
    stds = list()
    correct = 0
    NN = 0
    NA = 0
    AA = 0
    AN = 0
    labels = list()
    for i in range(len(te_data)):
        if not os.path.exists('.\\input-activations_' + strategy + '_' + str(n_sampled_paths) + '/' + str(i)):
            os.mkdir('.\\input-activations_' + strategy + '_' + str(n_sampled_paths) + '/' + str(i))
        if not os.path.exists('.\\forget-activations_' + strategy + '_' + str(n_sampled_paths) + '/' + str(i)):
            os.mkdir('.\\forget-activations_' + strategy + '_' + str(n_sampled_paths) + '/' + str(i))
        if not os.path.exists('.\\test-files_' + strategy + '_' + str(n_sampled_paths) + '/' + str(i)):
            os.mkdir('.\\test-files_' + strategy + '_' + str(n_sampled_paths) + '/' + str(i))

        te_x = te_data[i]
        te_y = te_labels[i]
        te_l = te_lens[i]

        te_x, te_l, te_max_len = padding4(te_x, te_l, encode_len)
        te_x = te_x.reshape((-1, int(te_max_len / encoded_len), encoded_len))
        te_l = (te_l / encoded_len).astype(np.int32)
        actual = int(te_y[0, 0])

        logits, cands, forgets = sess.run([model.logits, model.cands, model.forgets], feed_dict={model.x:te_x, model.actual_len:te_l})

        logits_tmp = logits.reshape((-1)).tolist()
        logits_tmp.append(actual)
        logits_list.append(logits_tmp)

        np.save('.\\input-activations_' + strategy + '_' + str(n_sampled_paths) + '/' + str(i) + '/data', cands)
        np.save('.\\forget-activations_' + strategy + '_' + str(n_sampled_paths) + '/' + str(i) + '/data', forgets)
        np.save('.\\test-files_' + strategy + '_' + str(n_sampled_paths) + '/' + str(i) + '/data', te_x)

        confidence = np.mean(logits.reshape((-1)))
        std = np.std(logits.reshape((-1)))

        confidences.append(confidence)
        stds.append(std)

        pred = int(np.round(confidence))
        labels.append(actual)

        print(strategy,' [', k, ']', i, 'actual: ', actual, ', pred: ', pred)
        if pred == actual: correct += 1
        if actual == 0 and pred == 0: NN += 1
        elif actual == 0 and pred == 1: NA += 1
        elif actual == 1 and pred == 0: AN += 1
        elif actual == 1 and pred == 1: AA += 1

    print()

    logits_list = np.array(logits_list)
    np.save(os.path.join('.\\logits_' + strategy + '_' + str(n_sampled_paths), 'logits_' + str(k)), logits_list)

    accuracy = float(correct) / float(len(te_data))
    precision = float(AA) / float(AA + NA) if AA + NA > 0 else 0.
    TPR = float(AA) / float(AA + AN) if AA + AN > 0 else 0.
    FPR = float(NA) / float(NA + NN) if NA + NN > 0 else 0.
    TNR = float(NN) / float(NA + NN) if NA + NN > 0 else 0.
    FNR = float(AN) / float(AA + AN) if AA + AN > 0 else 0.

    if not os.path.exists('.\\result_' + strategy + '_' + str(n_sampled_paths)):
        os.mkdir('.\\result_' + strategy + '_' + str(n_sampled_paths))

    f = open('.\\result_' + strategy + '_' + str(n_sampled_paths) + '/test_result_' + str(k) + '.txt', 'w')
    f.write('[' + str(k) + '] stratege: ' + strategy + '\n')
    f.write('[' + str(k) + '] accuracy: ' + str(accuracy) + '\n')
    f.write('[' + str(k) + '] precision: ' + str(precision) + '\n')
    f.write('[' + str(k) + '] True Positive Rate: ' + str(TPR) + '\n')
    f.write('[' + str(k) + '] False Positive Rate: ' + str(FPR) + '\n')
    f.write('[' + str(k) + '] True Negative Rate: ' + str(TNR) + '\n')
    f.write('[' + str(k) + '] False Negative Rate: ' + str(FNR) + '\n')
    f.write('[' + str(k) + '] Confusion Matrix\n')
    f.write('\t\t|\tactual A\t|\tactual N\n')
    f.write('========================================\n')
    f.write('pred A\t|\t\t' + str(AA) + '\t\t|\t\t' + str(NA) + '\n')
    f.write('========================================\n')
    f.write('pred N\t|\t\t' + str(AN) + '\t\t|\t\t' + str(NN) + '\n')
    f.write('[-] confidences\n')
    f.write(str(confidences) + '\n')
    f.write('[-] stds\n')
    f.write(str(stds) + '\n')
    f.write('[-] labels\n')
    f.write(str(labels))
    f.close()

    print('[*] stratege: ', strategy)
    print('[', k, '] accuracy: ', accuracy)
    print('[', k, '] precision: ', precision)
    print('[', k, '] True Positive Rate: ', TPR)
    print('[', k, '] False Positive Rate: ', FPR)
    print('[', k, '] True Negative Rate: ', TNR)
    print('[', k, '] False Negative Rate: ', FNR)
    print('[', k, '] Confusion Matrix')
    print('\t\t|\tactual A\t|\tactual N')
    print('========================================')
    print('pred A\t|\t\t', AA, '\t\t|\t\t', NA)
    print('========================================')
    print('pred N\t|\t\t', AN, '\t\t|\t\t', NN)

def run(n_sampled_path,
        do_what,
        data_root,
        strategy,
        model_path,
        plot,
        K,
        encode_len,
        n_embed,
        n_rnn_hidden,
        n_out,
        ae_lr,
        rnn_lr,
        n_update_ae,
        n_update_rnn,
        n_minibatch):

    data_root = data_root + '_' + strategy
    model_path = model_path + '_' + strategy
    # tr_data, tr_labels, te_data, te_labels = load_data(data_root, n_sampled_path, encode_len)
    data, labels, actual_lens = load_data(data_root, n_sampled_path, encode_len)

    print(data.shape)
    print(labels.shape)

    top_inst = load_top_inst()
    n_voca = len(top_inst)
    max_len = 1000 / encode_len
    fold_size = int(len(data) / K)

    if not os.path.exists('.\\result_' + strategy):
        os.mkdir('.\\result_' + strategy)

    for k in range(K):
        te_data = data[k * fold_size : (k+1) * fold_size]
        te_labels = labels[k * fold_size : (k+1) * fold_size]
        te_lens = actual_lens[k * fold_size : (k+1) * fold_size]
        tr_data = np.concatenate((data[(k-1)*fold_size : k * fold_size], data[(k+1) * fold_size:]), 0)
        tr_labels = np.concatenate((labels[(k-1)*fold_size : k * fold_size], labels[(k+1) * fold_size:]), 0)
        tr_lens = np.concatenate((actual_lens[(k-1)*fold_size : k * fold_size], actual_lens[(k+1) * fold_size:]), 0)

        with tf.Session() as sess:
            model = DetectionModel(encode_len, n_voca, n_embed, n_rnn_hidden)
            model.build()

            if do_what == 'all' or do_what == 'train':
                train(sess, model, tr_data, tr_labels, tr_lens, n_voca, encode_len, n_minibatch, ae_lr, rnn_lr, model_path, n_update_ae, n_update_rnn, strategy, k)
            if do_what == 'all' or do_what == 'train_lc':
                train_linear_classifier(sess, model, tr_data, tr_labels, tr_lens, n_voca, encode_len, n_minibatch, model_path, strategy, k, n_sampled_path)
            if do_what == 'all' or do_what == 'test':
                test(sess, model, te_data, te_labels, te_lens, n_sampled_path, encode_len, model_path, strategy, k)

        tf.reset_default_graph()
